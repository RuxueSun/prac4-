#################### 1 netup function
## purpose:
## Input:
##      d:a vector giving the number of nodes in each layer of a network.
## Output:
##      h: a list of nodes for each layer.
##      w: a list of weight matrices.
##      b: a list of offset vectors.

netup <- function(d){
  
  # calculate the number of layers
  layer <- length(d)
  
  # Initialize weight matrices with U(0, 0.2) random deviates
  W <- vector('list', layer-1)
  for (i in 1:(layer-1)){
    W[[i]] <- matrix(runif(d[i] * d[i+1],0 , 0.2), nrow = d[i+1], ncol = d[i])
  }
  
  # Initialize offset vectors with U(0, 0.2) random deviates
  b <- vector('list', layer-1)
  for (i in 1:(layer-1)){
    b[[i]] <- runif(d[i+1],0,0.2)
  }
  
  # Initialize nodes for each layer
  h <- vector('list', layer)
  
  # nodes for first layer is the values of input data
  h[[1]] <- numeric(d[1])
  
  # nodes for other layers
  for (i in 2:layer){
    for (j in 1:d[i]){
      h[[i]][j] <- max(0,W[[i-1]][j,] %*% h[[i-1]] + b[[i-1]][j])
    }
  }
  
  network <- list(h=h, W=W, b=b)
  
  return(network)
  
}

#################### 2 forward function
## purpose:
## Input:
##       nn: a network list as returned by netup.
##       inp: a vector of input values for the first layer.
## Output:
##       updated network list.

forward <- function(nn,inp){
  
  # recall the results of netup function
  h <- nn$h
  W <- nn$W 
  b <- nn$b 
  
  # values for the first layer
  h[[1]] <- inp
  
  # nodes for other layers
  for (i in 2:length(W)+1){
    h[[i]] <- pmax(0, W[[i-1]] %*% h[[i-1]] + b[[i-1]])
  }
  
  network <- list(h=h, W=W, b=b)
  
  return(network)
}

#################### 3 backward function
## purpose:
## Input:
##       nn: a network list as returned by forward.
##       k: output class.
## Output:
##       updated network list after adding dh, dW, db.

backword <- function(nn,k){
  
  # recall the results of forward function
  h <- nn$h
  W <- nn$W 
  b <- nn$b
  
  # calculate the layers
  layer <- length(h)
  
  # 
  dh <- vector('list', layer)
  dw <- vector('list', layer-1)
  db <- vector('list', layer-1)
  
  # pk
  #求和
  sum = 0
  for (i in 1:length(h[[layer]])){
    sum <- sum + exp(h[[layer]][i])
  }
  
  pk <- exp(h[[layer]][k])/sum
  
  # loss function
  L_i <- -log(pk)
  
  # dh的最后一个
  for (j in 1:length(h[[layer]])){
    if (j == k){
      dh[[layer]][j] <- exp(h[[layer]][j])/sum
    }else{
      dh[[layer]][j] <- exp(h[[layer]][j])/(sum - 1)
    }
  }
  
  # 求dl
  dl <- vector('list', layer)
  
  # 给dl第一个列表赋值 只用到第二到layer个
  dl[[1]] <- numeric(length(h[[1]]))
  
  # 求dl的2到layer个列表
  for (i in (layer-1):1){
    for (j in length(h[[i+1]])){
      if (h[[i+1]][j] > 0){
        dl[[i+1]][j] <- dh[[i+1]][j]
      }else{
        dl[[i+1]][j] <- 0
      }
    }
  }
  
  # 求dh的其他
  for (i in 1:(layer-1)){
    dh[[i]] <- t(W[[i]]) %*% dl[[i+1]]
  }
  
  # 求对b的偏导
  for (i in 1:(layer-1)){
    db[[i]] <- dl[[i+1]]
  }
  
  # 求对W的偏导
  for (i in 1:(layer-1)){
    dW[[i]] <- dl[[i+1]] %*% t(h[[i]])
  }
  
  network <- list(h=h, W=W, b=b, dh=dh, dW=dW, db=db)
  
  return(network)
  
}

#################### 4 train function
## purpose:
## Input:
##       nn: a network list as returned by backwork.
##       inp: given input data in the rows of matrix.
##       k: a vector shows corresponding labels (1, 2, 3 . . . ).
##       eta: the step size.
##       mb: the number of data to randomly sample to compute the gradient.
##       nstep: the number of optimization steps to take.
## Output:
##       

train <- function(nn, inp, k, eta = 0.01, mb = 10, nstep = 10000) {
  n <- nrow(inp)  
  for (step in 1:nstep) {
    indices <- sample(1:n, mb, replace = TRUE) 
    mini_batch_inp <- inp[indices, ] 
    mini_batch_k <- k[indices]
    nn <- forward(nn, mini_batch_inp)
    nn <- backward(nn, mini_batch_k)
    nn <- update_parameters(nn, eta)
  }
  return(nn)
}


