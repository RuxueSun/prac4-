netup <- function(d){
  
  # calculate the number of layers
  layer <- length(d)
  
  # Initialize weight matrices with U(0, 0.2) random deviates
  W <- vector('list', layer-1)
  for (i in 1:(layer-1)){
    W[[i]] <- matrix(runif(d[i] * d[i+1],0 , 0.2), nrow = d[i+1], ncol = d[i])
  }
  
  # Initialize offset vectors with U(0, 0.2) random deviates
  b <- vector('list', layer-1)
  for (i in 1:(layer-1)){
    b[[i]] <- runif(d[i+1],0,0.2)
  }
  
  # Initialize nodes for each layer
  h <- lapply(d, function(k) rep(0, k))
  
  network <- list(h=h, W=W, b=b)
  
  return(network)
  
}


forward <- function(nn,inp){
  
  # recall the results of netup function
  h <- nn$h
  W <- nn$W 
  b <- nn$b 
  
  # values for the first layer
  h[[1]] <- inp
  
  # nodes for other layers
  for (i in 2:length(W)+1){
    h[[i]] <- pmax(0, W[[i-1]] %*% h[[i-1]] + b[[i-1]])
  }
  
  network <- list(h=h, W=W, b=b)
  
  return(network)
}


backward <- function(nn,k){
  
  h <- nn$h
  W <- nn$W 
  b <- nn$b
  
  # calculate the layers
  layer <- length(h)
  
  # 
  dh <- vector('list', layer)
  dW <- vector('list', layer-1)
  db <- vector('list', layer-1)
  
  # pk
  #求和
  sum <- sum(exp(h[[layer]]))
  
  pk <- exp(h[[layer]][k])/sum
  
  # loss function
  L_i <- -log(pk)
  
  # dh的最后一个
  for (j in 1:length(h[[layer]])){
    if (j != k){
      dh[[layer]][j] <- exp(h[[layer]][j])/sum
    }else{
      dh[[layer]][j] <- exp(h[[layer]][j])/sum - 1 ####?分母
    }
  }
  
  # 求dl
  db <- vector('list', layer-1)
  
  # 求dl的2到layer个列表和dh的其他
  for (i in layer:2){
    
    for (j in 1:length(h[[i]])){
      
      if (h[[i]][j] > 0 ){
        db[[i-1]][j] <- dh[[i]][j]
      }else{
        db[[i-1]][j] <- 0
      }
      
    }
    
    dh[[i-1]] <- t(W[[i-1]]) %*% db[[i-1]]
    
  }
  
  # 求对W的偏导
  for (i in 1:(layer-1)){
    bdb <- as.numeric(db[[i]])
    hnn <- as.numeric(h[[i]])
    dW[[i]] <- bdb %*% t(hnn)
  }
  
  
  network <- list(h=h, W=W, b=b, dh=dh, dW=dW, db=db )
  
  return(network)
  
}


##############4
train <- function(nn, inp, k, eta = 0.01, mb = 10, nstep = 10000) {
  
  #recursive to optimize the network
  for(j in 1:nstep){
    sp <- sample(1:nrow(inp),mb)
    
    sumW <- lapply(nn$W, function(mat) matrix(0, nrow = nrow(mat), 
                                              ncol = ncol(mat)))
    sumb <- lapply(nn$b, function(vec) matrix(0, nrow = length(vec),
                                              ncol = 1))
    
    for(i in 1:mb){
      #第一次，输入数据到网络
      nn <- forward(nn,inp[sp[i],])
      nb <- backward(nn,k[sp[i]])
      
      #求和
      sumW <- lapply(seq_along(sumW), function(i) sumW[[i]] + nb$dW[[i]])
      sumb <- Map('+',sumb,nb$db)
    }
    
    eta_EW <- lapply(sumW, function(a) eta*(a/10) )
    eta_Eb <- lapply(sumb, function(a) eta*(a/10))
    
    #更新
    nn$W <- lapply(seq_along(eta_EW), function(i) nn$W[[i]] - eta_EW[[i]])
    nn$b <- Map('-',nn$b,eta_Eb)
    
  }
  
  return(nn)
  
}

##############5
set.seed(11)
data(iris)
k <- c(4,8,7,3)
iris$Species_numeric <- as.numeric(iris$Species) #类别数值化
iris <- as.matrix(iris[,-5])
num_rows <- nrow(iris)
test_index <- seq(5,num_rows,5)
test_data <- iris[test_index,]

training_data <- iris[-test_index,]

#train:
nn <- netup(k)
#backward(forward(nn,iris[1,1:4]),1)
test <- train(nn, training_data[,1:4], training_data[,5],
              eta = 0.01, mb = 10, nstep = 10000)
